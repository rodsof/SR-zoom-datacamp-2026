{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "399bc202",
   "metadata": {},
   "source": [
    "### Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b70b04f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, unix_timestamp, col, max as spark_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651d70db",
   "metadata": {},
   "source": [
    "### Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9e29d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/26 14:00:13 WARN Utils: Your hostname, LAR-SRODRIGUEZ resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "26/02/26 14:00:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.4-amzn-0\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"test\").getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18cf490",
   "metadata": {},
   "source": [
    "### Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5088398f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descarga completada: yellow_tripdata_2025-11.parquet\n"
     ]
    }
   ],
   "source": [
    "url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-11.parquet\"\n",
    "output_file = \"yellow_tripdata_2025-11.parquet\"\n",
    "\n",
    "with requests.get(url, stream=True) as r:\n",
    "    r.raise_for_status()\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        for chunk in r.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "print(\"Descarga completada:\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e632ba95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:==========================================>              (9 + 3) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|cbd_congestion_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
      "|       2| 2025-11-02 08:11:08|  2025-11-02 08:15:21|              1|         1.24|         1|                 N|         186|         230|           1|        7.9|  0.0|    0.5|      2.53|         0.0|                  1.0|       15.18|                 2.5|        0.0|              0.75|\n",
      "|       2| 2025-11-06 14:01:48|  2025-11-06 14:25:53|              2|         1.84|         1|                 N|         164|         237|           2|       20.5|  0.0|    0.5|       0.0|         0.0|                  1.0|       25.25|                 2.5|        0.0|              0.75|\n",
      "|       2| 2025-11-07 16:53:08|  2025-11-07 17:10:10|              1|         1.15|         1|                 N|         186|         161|           1|       14.9|  2.5|    0.5|      0.04|         0.0|                  1.0|       22.19|                 2.5|        0.0|              0.75|\n",
      "|       2| 2025-11-09 10:55:05|  2025-11-09 10:58:57|              1|         0.55|         1|                 N|          68|         246|           1|        5.8|  0.0|    0.5|      2.11|         0.0|                  1.0|       12.66|                 2.5|        0.0|              0.75|\n",
      "|       2| 2025-11-03 13:35:09|  2025-11-03 13:47:48|              1|         0.91|         1|                 N|          90|         164|           1|       12.1|  0.0|    0.5|      3.37|         0.0|                  1.0|       20.22|                 2.5|        0.0|              0.75|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read November 2025 Yellow CSV into a Spark DataFrame\n",
    "input_path = output_file  # Update path if needed\n",
    "df = spark.read.option('header', True).parquet(input_path)\n",
    "\n",
    "# Repartition to 4 partitions\n",
    "df_repart = df.repartition(4)\n",
    "\n",
    "# Save to Parquet\n",
    "output_path = '/home/hadoop/workspace/notebooks/yellow_november_2025_parquet'  # Update path if needed\n",
    "df_repart.write.mode('overwrite').parquet(output_path)\n",
    "\n",
    "df_repart.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448c10f1",
   "metadata": {},
   "source": [
    "### Count records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fc66122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "162604"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(\n",
    "    (col(\"tpep_pickup_datetime\") >= \"2025-11-15\") &\n",
    "    (col(\"tpep_pickup_datetime\") < \"2025-11-16\")\n",
    ").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8ca08a",
   "metadata": {},
   "source": [
    "### Longest trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd74fa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_trip_hours = df.withColumn(\n",
    "    \"trip_hours\",\n",
    "    (unix_timestamp(\"tpep_dropoff_datetime\") - unix_timestamp(\"tpep_pickup_datetime\")) / 3600\n",
    ").agg(\n",
    "    spark_max(\"trip_hours\").alias(\"longest_trip_hours\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67e45f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|longest_trip_hours|\n",
      "+------------------+\n",
      "| 90.64666666666666|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "longest_trip_hours.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be68acd1",
   "metadata": {},
   "source": [
    "###  Least frequent pickup location zone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2e4712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|PULocationID|count|\n",
      "+------------+-----+\n",
      "|           5|    1|\n",
      "+------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "least_frequent_pickup_zone = df.groupBy(\"PULocationID\") \\\n",
    "  .count() \\\n",
    "  .orderBy(\"count\", ascending=True) \\\n",
    "  .show(1) # --> PULocationID 5 has the least pickups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3f7112",
   "metadata": {},
   "source": [
    "#### Download CSV zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eef21b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-02-26 14:29:30--  https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\n",
      "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 3.163.140.127, 3.163.140.18, 3.163.140.37, ...\n",
      "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|3.163.140.127|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12331 (12K) [text/csv]\n",
      "Saving to: ‘taxi_zone_lookup.csv’\n",
      "\n",
      "taxi_zone_lookup.cs 100%[===================>]  12.04K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2026-02-26 14:29:31 (10.8 MB/s) - ‘taxi_zone_lookup.csv’ saved [12331/12331]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv -O taxi_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b5ec99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+-----+\n",
      "|PULocationID|         Zone|count|\n",
      "+------------+-------------+-----+\n",
      "|           5|Arden Heights|    1|\n",
      "+------------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the lookup CSV\n",
    "zone_df = spark.read.option(\"header\", True).csv(\"taxi_zone_lookup.csv\")\n",
    "\n",
    "# Find least frequent pickup zone\n",
    "least_frequent_pickup_zone = df.groupBy(\"PULocationID\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"count\", ascending=True) \\\n",
    "    .limit(1)\n",
    "\n",
    "# Join with zone lookup to get the zone name\n",
    "result = least_frequent_pickup_zone.join(\n",
    "    zone_df,\n",
    "    least_frequent_pickup_zone.PULocationID == zone_df.LocationID,\n",
    "    how=\"left\"\n",
    ").select(\n",
    "    least_frequent_pickup_zone.PULocationID,\n",
    "    \"Zone\",\n",
    "    \"count\"\n",
    ")\n",
    "\n",
    "result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
