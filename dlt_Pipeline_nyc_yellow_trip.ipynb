{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b69397c1",
   "metadata": {},
   "source": [
    "# Building a Data Pipeline for NYC Yellow Trip Data\n",
    "\n",
    "In this notebook, we will build a complete data pipeline using **dlt** to process NYC Yellow Taxi Trip data.\n",
    "\n",
    "Our goal is:\n",
    "\n",
    "‚Üí Fetch real trip data from the Data Engineering Zoomcamp API  \n",
    "‚Üí Turn it into clean relational tables  \n",
    "‚Üí Load it into DuckDB  \n",
    "‚Üí Explore and analyze it  \n",
    "\n",
    "We will use the **Data Engineering Zoomcamp API** as our data source and **DuckDB** as our database.\n",
    "\n",
    "Along the way, you will learn:\n",
    "\n",
    "- How to configure a paginated REST API source  \n",
    "- How to handle pagination that stops on empty pages  \n",
    "- How the Extract ‚Üí Normalize ‚Üí Load process works  \n",
    "- How to inspect and explore NYC taxi trip data  \n",
    "\n",
    "By the end, you will have a working pipeline processing real-world transportation data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7474c8a",
   "metadata": {},
   "source": [
    "## üì¶ Step 0: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6e9ad1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install dependencies first\n",
    "!pip -q install dlt[duckdb]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd43365",
   "metadata": {},
   "source": [
    "In this notebook we will use:\n",
    "\n",
    "- **dlt** to extract, normalize, and load data\n",
    "- **DuckDB** as the destination database (runs locally)\n",
    "\n",
    "DuckDB is great for data analysis because it requires no setup and no credentials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66efd41",
   "metadata": {},
   "source": [
    "## üìö Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808141fb",
   "metadata": {},
   "source": [
    "In this cell we import the libraries we will use throughout the notebook:\n",
    "\n",
    "- **dlt** is the main library for building and running the pipeline\n",
    "- **rest_api_source** helps us define an API source using a simple configuration\n",
    "- **islice** (from `itertools`) is a small Python helper for previewing only a few records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9777aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "from itertools import islice\n",
    "from dlt.sources.rest_api import rest_api_source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc5a147",
   "metadata": {},
   "source": [
    "## üîó Step 2: Define the API Source (NYC Yellow Trip Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0692540",
   "metadata": {},
   "source": [
    "In **dlt**, a **source** is the part of your pipeline that knows how to fetch data from somewhere.\n",
    "In this notebook, our source fetches NYC Yellow Taxi trip data from the **Data Engineering Zoomcamp API**.\n",
    "\n",
    "We define the source using `rest_api_source`, which lets us describe an API in a simple Python dictionary.\n",
    "\n",
    "**API Specifications:**\n",
    "- **Base URL:** https://us-central1-dlthub-analytics.cloudfunctions.net/data_engineering_zoomcamp_api\n",
    "- **Format:** Paginated JSON\n",
    "- **Page Size:** 1,000 records per page\n",
    "- **Pagination:** Stops when an empty page is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62ca9f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nyc_yellow_trip_source():\n",
    "    \"\"\"\n",
    "    Creates a dlt source that fetches NYC Yellow Taxi trip data\n",
    "    from the Data Engineering Zoomcamp API.\n",
    "    \"\"\"\n",
    "    return rest_api_source({\n",
    "        \"client\": {\n",
    "            \"base_url\": \"https://us-central1-dlthub-analytics.cloudfunctions.net\",\n",
    "        },\n",
    "        \"resources\": [\n",
    "            {\n",
    "                \"name\": \"yellow_trips\",\n",
    "                \"endpoint\": {\n",
    "                    \"path\": \"data_engineering_zoomcamp_api\",\n",
    "                    \"paginator\": {\n",
    "                        \"type\": \"page_number\",\n",
    "                        \"page_param\": \"page\",\n",
    "                        \"total_path\": None,  # Stop on empty page\n",
    "                        \"base_page\": 1,\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2c9e27",
   "metadata": {},
   "source": [
    "## üîß Step 3: Create the dlt Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45865774",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"nyc_yellow_trip_pipeline\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"nyc_taxi_data\",\n",
    "    progress=\"log\"  # logs the pipeline run (Optional)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5ccff8",
   "metadata": {},
   "source": [
    "## üîç Understanding the Pipeline\n",
    "\n",
    "At this point we have defined two key building blocks:\n",
    "\n",
    "- **The source** describes where the data comes from and how to fetch it from the API.  \n",
    "- **The pipeline** describes where the data should go (DuckDB) and keeps track of tables, schemas, and run history.  \n",
    "\n",
    "---\n",
    "\n",
    "Instead of running everything at once, we will now run the pipeline in three separate phases so you can clearly see what happens at each stage:\n",
    "\n",
    "1. **Extract**: download raw data from the API  \n",
    "2. **Normalize**: turn nested JSON into relational tables  \n",
    "3. **Load**: write those tables into DuckDB  \n",
    "\n",
    "Once these steps make sense, we will run the full workflow again using one command:\n",
    "\n",
    "```python\n",
    "pipeline.run(source)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b97129",
   "metadata": {},
   "source": [
    "## ‚¨áÔ∏è Step 4: Extract\n",
    "\n",
    "Now we run the first stage of the pipeline: **Extract**.\n",
    "\n",
    "Extract means:\n",
    "\n",
    "- dlt sends requests to the Data Engineering Zoomcamp API\n",
    "- the raw JSON responses are downloaded\n",
    "- the results are stored in dlt's local working folder\n",
    "\n",
    "At this stage, the data is **not** in DuckDB yet. We are just confirming that we successfully pulled data from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba4f467b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------- Extract rest_api -------------------------------\n",
      "Resources: 0/1 (0.0%) | Time: 0.00s | Rate: 0.00/s\n",
      "Memory usage: 177.91 MB (33.40%) | CPU usage: 0.00%\n",
      "\n",
      "------------------------------- Extract rest_api -------------------------------\n",
      "Resources: 0/1 (0.0%) | Time: 2.94s | Rate: 0.00/s\n",
      "yellow_trips: 1000  | Time: 0.00s | Rate: 127100121.21/s\n",
      "Memory usage: 180.04 MB (33.40%) | CPU usage: 0.00%\n",
      "\n",
      "------------------------------- Extract rest_api -------------------------------\n",
      "Resources: 0/1 (0.0%) | Time: 5.78s | Rate: 0.00/s\n",
      "yellow_trips: 2000  | Time: 2.84s | Rate: 703.77/s\n",
      "Memory usage: 181.79 MB (32.80%) | CPU usage: 0.00%\n",
      "\n",
      "------------------------------- Extract rest_api -------------------------------\n",
      "Resources: 0/1 (0.0%) | Time: 8.60s | Rate: 0.00/s\n",
      "yellow_trips: 3000  | Time: 5.67s | Rate: 529.30/s\n",
      "Memory usage: 182.91 MB (32.80%) | CPU usage: 0.00%\n",
      "\n",
      "------------------------------- Extract rest_api -------------------------------\n",
      "Resources: 0/1 (0.0%) | Time: 11.49s | Rate: 0.00/s\n",
      "yellow_trips: 4000  | Time: 8.55s | Rate: 467.70/s\n",
      "Memory usage: 184.16 MB (32.90%) | CPU usage: 0.00%\n",
      "\n",
      "------------------------------- Extract rest_api -------------------------------\n",
      "Resources: 0/1 (0.0%) | Time: 14.28s | Rate: 0.00/s\n",
      "yellow_trips: 5000  | Time: 11.34s | Rate: 440.73/s\n",
      "Memory usage: 185.99 MB (32.80%) | CPU usage: 0.00%\n",
      "\n",
      "------------------------------- Extract rest_api -------------------------------\n",
      "Resources: 0/1 (0.0%) | Time: 16.99s | Rate: 0.00/s\n",
      "yellow_trips: 6000  | Time: 14.05s | Rate: 427.04/s\n",
      "Memory usage: 185.99 MB (32.80%) | CPU usage: 0.00%\n",
      "\n",
      "------------------------------- Extract rest_api -------------------------------\n",
      "Resources: 0/1 (0.0%) | Time: 19.27s | Rate: 0.00/s\n",
      "yellow_trips: 7000  | Time: 16.33s | Rate: 428.54/s\n",
      "Memory usage: 185.99 MB (32.80%) | CPU usage: 0.00%\n",
      "\n",
      "------------------------------- Extract rest_api -------------------------------\n",
      "Resources: 0/1 (0.0%) | Time: 21.03s | Rate: 0.00/s\n",
      "yellow_trips: 8000  | Time: 18.10s | Rate: 442.02/s\n",
      "Memory usage: 186.12 MB (32.80%) | CPU usage: 0.00%\n",
      "\n",
      "------------------------------- Extract rest_api -------------------------------\n",
      "Resources: 0/1 (0.0%) | Time: 23.36s | Rate: 0.00/s\n",
      "yellow_trips: 9000  | Time: 20.43s | Rate: 440.62/s\n",
      "Memory usage: 186.12 MB (32.80%) | CPU usage: 0.00%\n",
      "\n",
      "------------------------------- Extract rest_api -------------------------------\n",
      "Resources: 0/1 (0.0%) | Time: 25.76s | Rate: 0.00/s\n",
      "yellow_trips: 10000  | Time: 22.83s | Rate: 438.06/s\n",
      "Memory usage: 187.62 MB (32.80%) | CPU usage: 0.00%\n",
      "\n",
      "------------------------------- Extract rest_api -------------------------------\n",
      "Resources: 0/1 (0.0%) | Time: 27.73s | Rate: 0.00/s\n",
      "yellow_trips: 10000  | Time: 24.79s | Rate: 403.32/s\n",
      "Memory usage: 187.62 MB (32.80%) | CPU usage: 0.00%\n",
      "\n",
      "------------------------------- Extract rest_api -------------------------------\n",
      "Resources: 1/1 (100.0%) | Time: 27.74s | Rate: 0.04/s\n",
      "yellow_trips: 10000  | Time: 24.81s | Rate: 403.13/s\n",
      "Memory usage: 187.62 MB (32.80%) | CPU usage: 0.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extract_info = pipeline.extract(nyc_yellow_trip_source())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb1574d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### What we will print\n",
    "\n",
    "After extraction, we will print a small summary showing:\n",
    "\n",
    "- which **resources** were extracted\n",
    "- which **tables** will be created later\n",
    "- how many rows were extracted per resource\n",
    "\n",
    "This helps confirm that the pipeline is working before we move on to normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20ebd792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resources: ['yellow_trips']\n",
      "Tables: ['yellow_trips']\n",
      "Load ID: 1772214533.8725717\n",
      "\n",
      "Resource: yellow_trips\n",
      "rows extracted: 10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "load_id = extract_info.loads_ids[-1]\n",
    "m = extract_info.metrics[load_id][0]\n",
    "\n",
    "print(\"Resources:\", list(m[\"resource_metrics\"].keys()))\n",
    "print(\"Tables:\", list(m[\"table_metrics\"].keys()))\n",
    "print(\"Load ID:\", load_id)\n",
    "print()\n",
    "\n",
    "for resource, rm in m[\"resource_metrics\"].items():\n",
    "    print(f\"Resource: {resource}\")\n",
    "    print(f\"rows extracted: {rm.items_count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f946efd2",
   "metadata": {},
   "source": [
    "### What you should see after Extract\n",
    "\n",
    "After extraction, you should see:\n",
    "\n",
    "- **Resources:** `['yellow_trips']`  \n",
    "- **Tables:** `['yellow_trips']`\n",
    "\n",
    "The number of rows extracted depends on how many taxi trip records are available from the API. Each page contains up to 1,000 records, and the pagination continues until an empty page is returned.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa9f635",
   "metadata": {},
   "source": [
    "## üîÑ Step 5: Normalize\n",
    "\n",
    "Now we run **Normalize**. This is where dlt transforms raw JSON into a clean relational structure.\n",
    "\n",
    "During normalization, dlt does three key things:\n",
    "\n",
    "### 1. Adds Tracking Columns to the Main Table\n",
    "\n",
    "dlt adds special columns to every table:\n",
    "- `_dlt_id`: A unique identifier for each row\n",
    "- `_dlt_load_id`: Links each row to the load job that created it\n",
    "\n",
    "### 2. Flattens Nested Data into Child Tables\n",
    "\n",
    "If the API response contains nested structures (like arrays or objects), dlt will flatten them into separate child tables with names like:\n",
    "- `yellow_trips__nested_field`\n",
    "\n",
    "Each child table has a `_dlt_parent_id` column that references `_dlt_id` in the parent table.\n",
    "\n",
    "### 3. Creates Metadata Tables\n",
    "\n",
    "dlt also creates internal tables to track pipeline state:\n",
    "- `_dlt_loads`: Tracks load history (when data was loaded, status)\n",
    "- `_dlt_pipeline_state`: Stores pipeline state for incremental loading\n",
    "- `_dlt_version`: Tracks schema versions\n",
    "\n",
    "In the next cell, we will print a summary showing which tables were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b756d25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- Normalize rest_api in 1772214256.4101622 -------------------\n",
      "Files: 0/1 (0.0%) | Time: 0.00s | Rate: 0.00/s\n",
      "Memory usage: 187.74 MB (32.90%) | CPU usage: 0.00%\n",
      "\n",
      "------------------- Normalize rest_api in 1772214256.4101622 -------------------\n",
      "Files: 0/1 (0.0%) | Time: 0.00s | Rate: 0.00/s\n",
      "Items: 0  | Time: 0.00s | Rate: 0.00/s\n",
      "Memory usage: 187.74 MB (32.90%) | CPU usage: 0.00%\n",
      "\n",
      "------------------- Normalize rest_api in 1772214256.4101622 -------------------\n",
      "Files: 1/1 (100.0%) | Time: 0.11s | Rate: 9.39/s\n",
      "Items: 1000  | Time: 0.11s | Rate: 9452.21/s\n",
      "Memory usage: 187.93 MB (32.90%) | CPU usage: 0.00%\n",
      "\n"
     ]
    },
    {
     "ename": "PipelineStepFailed",
     "evalue": "Pipeline execution failed at `step=normalize` when processing package with `load_id=1772214256.4101622` with exception:\n\n<class 'dlt.common.schema.exceptions.UnboundColumnException'>\nIn schema `rest_api`: The following columns in table `yellow_trips` did not receive any data during this load:\n  - id (marked as non-nullable primary key and must have values)\n\nThis can happen if you specify columns manually, for example, using the `merge_key`, `primary_key` or `columns` argument but they do not exist in the data.\n\n\nPending packages are left in the pipeline and will be re-tried on the next pipeline run. If you pass new data to extract to next run, it will be ignored. Run `dlt pipeline nyc_yellow_trip_pipeline info` for more information or `dlt pipeline nyc_yellow_trip_pipeline drop-pending-packages` to drop pending packages.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundColumnException\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/dlt/pipeline/pipeline.py:553\u001b[39m, in \u001b[36mPipeline.normalize\u001b[39m\u001b[34m(self, workers)\u001b[39m\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[32m    549\u001b[39m     signals.intercepted_signals()\n\u001b[32m    550\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.runtime_config.intercept_signals\n\u001b[32m    551\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m nullcontext()\n\u001b[32m    552\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m     \u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalize_step\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_step_info(normalize_step)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/dlt/common/runners/pool_runner.py:208\u001b[39m, in \u001b[36mrun_pool\u001b[39m\u001b[34m(config, run_f)\u001b[39m\n\u001b[32m    207\u001b[39m logger.debug(\u001b[33m\"\u001b[39m\u001b[33mRunning pool\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[43m_run_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    209\u001b[39m     \u001b[38;5;66;03m# raise on signal: safe to do that out of _run_func()\u001b[39;00m\n\u001b[32m    210\u001b[39m     signals.raise_if_signalled()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/dlt/common/runners/pool_runner.py:201\u001b[39m, in \u001b[36mrun_pool.<locals>._run_func\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(run_f, Runnable):\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m     run_metrics = \u001b[43mrun_f\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTExecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/dlt/normalize/normalize.py:349\u001b[39m, in \u001b[36mNormalize.run\u001b[39m\u001b[34m(self, pool)\u001b[39m\n\u001b[32m    348\u001b[39m         \u001b[38;5;28mself\u001b[39m._step_info_start_load_id(load_id)\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspool_schema_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# return info on still pending packages (if extractor saved something in the meantime)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/dlt/normalize/normalize.py:303\u001b[39m, in \u001b[36mNormalize.spool_schema_files\u001b[39m\u001b[34m(self, load_id, schema, files)\u001b[39m\n\u001b[32m    302\u001b[39m map_f: TMapFuncType = \u001b[38;5;28mself\u001b[39m.map_parallel \u001b[38;5;28;01mif\u001b[39;00m workers > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.map_single\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspool_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdate_normalizers\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m load_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/dlt/normalize/normalize.py:261\u001b[39m, in \u001b[36mNormalize.spool_files\u001b[39m\u001b[34m(self, load_id, schema, map_f, files)\u001b[39m\n\u001b[32m    260\u001b[39m table = schema.tables[table_name]\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m \u001b[43mverify_normalized_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdestination_capabilities\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[38;5;28mself\u001b[39m.clean_x_normalizer(load_id, table_name, table, schema.naming.PATH_SEPARATOR)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/dlt/normalize/validate.py:88\u001b[39m, in \u001b[36mverify_normalized_table\u001b[39m\u001b[34m(schema, table, capabilities)\u001b[39m\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m UnboundColumnException(schema.name, table[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m], [column])\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m incomplete_nullable_not_seen_data:\n",
      "\u001b[31mUnboundColumnException\u001b[39m: In schema `rest_api`: The following columns in table `yellow_trips` did not receive any data during this load:\n  - id (marked as non-nullable primary key and must have values)\n\nThis can happen if you specify columns manually, for example, using the `merge_key`, `primary_key` or `columns` argument but they do not exist in the data.\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mPipelineStepFailed\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m normalize_info = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/dlt/pipeline/pipeline.py:224\u001b[39m, in \u001b[36mwith_runtime_trace.<locals>.decorator.<locals>._wrap\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trace:\n\u001b[32m    222\u001b[39m         trace_step = start_trace_step(trace, cast(TPipelineStep, f.\u001b[34m__name__\u001b[39m), \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     step_info = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m step_info\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/dlt/pipeline/pipeline.py:178\u001b[39m, in \u001b[36mwith_schemas_sync.<locals>._wrap\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28mself\u001b[39m._schema_storage.commit_live_schema(name)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     rv = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    180\u001b[39m     \u001b[38;5;66;03m# because we committed live schema before calling f, we may safely\u001b[39;00m\n\u001b[32m    181\u001b[39m     \u001b[38;5;66;03m# drop all changes in live schemas\u001b[39;00m\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m._schema_storage.live_schemas.keys()):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/dlt/pipeline/pipeline.py:272\u001b[39m, in \u001b[36mwith_config_section.<locals>.decorator.<locals>._wrap\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrap\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mPipeline\u001b[39m\u001b[33m\"\u001b[39m, *args: Any, **kwargs: Any) -> Any:\n\u001b[32m    266\u001b[39m     \u001b[38;5;66;03m# add section context to the container to be used by all configuration without explicit sections resolution\u001b[39;00m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inject_section(\n\u001b[32m    268\u001b[39m         ConfigSectionContext(\n\u001b[32m    269\u001b[39m             pipeline_name=\u001b[38;5;28mself\u001b[39m.pipeline_name, sections=sections, merge_style=merge_func\n\u001b[32m    270\u001b[39m         )\n\u001b[32m    271\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/dlt/pipeline/pipeline.py:561\u001b[39m, in \u001b[36mPipeline.normalize\u001b[39m\u001b[34m(self, workers)\u001b[39m\n\u001b[32m    559\u001b[39m     err_load_id = normalize_step.current_load_id\n\u001b[32m    560\u001b[39m step_info = \u001b[38;5;28mself\u001b[39m._get_step_info(normalize_step)\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m PipelineStepFailed(\n\u001b[32m    562\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    563\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnormalize\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    564\u001b[39m     err_load_id,\n\u001b[32m    565\u001b[39m     n_ex,\n\u001b[32m    566\u001b[39m     step_info,\n\u001b[32m    567\u001b[39m ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mn_ex\u001b[39;00m\n",
      "\u001b[31mPipelineStepFailed\u001b[39m: Pipeline execution failed at `step=normalize` when processing package with `load_id=1772214256.4101622` with exception:\n\n<class 'dlt.common.schema.exceptions.UnboundColumnException'>\nIn schema `rest_api`: The following columns in table `yellow_trips` did not receive any data during this load:\n  - id (marked as non-nullable primary key and must have values)\n\nThis can happen if you specify columns manually, for example, using the `merge_key`, `primary_key` or `columns` argument but they do not exist in the data.\n\n\nPending packages are left in the pipeline and will be re-tried on the next pipeline run. If you pass new data to extract to next run, it will be ignored. Run `dlt pipeline nyc_yellow_trip_pipeline info` for more information or `dlt pipeline nyc_yellow_trip_pipeline drop-pending-packages` to drop pending packages."
     ]
    }
   ],
   "source": [
    "normalize_info = pipeline.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9646f94f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m load_id = \u001b[43mnormalize_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      2\u001b[39m m = normalize_info.metrics[load_id][\u001b[32m0\u001b[39m]\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoad ID:\u001b[39m\u001b[33m\"\u001b[39m, load_id)\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "load_id = normalize_info.loads_ids[-1]\n",
    "m = normalize_info.metrics[load_id][0]\n",
    "\n",
    "print(\"Load ID:\", load_id)\n",
    "print()\n",
    "\n",
    "print(\"Tables created/updated:\")\n",
    "for table_name, tm in m[\"table_metrics\"].items():\n",
    "    # skip dlt internal tables to keep it beginner-friendly\n",
    "    if table_name.startswith(\"_dlt\"):\n",
    "        continue\n",
    "    print(f\"  - {table_name}: {tm.items_count} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde78eca",
   "metadata": {},
   "source": [
    "### What happened during Normalize?\n",
    "\n",
    "After running `pipeline.normalize()`, the data has been transformed from raw JSON into a **relational structure**.\n",
    "\n",
    "If the NYC Yellow Trip data contains nested fields, you may see additional child tables created automatically by dlt.\n",
    "\n",
    "---\n",
    "\n",
    "### Schema Visualization\n",
    "\n",
    "dlt can render the schema as a visual diagram. Run the next cell to see the table relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746b93cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Display schema\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mpipeline\u001b[49m.default_schema\n",
      "\u001b[31mNameError\u001b[39m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# Display schema\n",
    "pipeline.default_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ba9f54",
   "metadata": {},
   "source": [
    "## üì§ Step 6: Load\n",
    "\n",
    "Now we run the final stage of the pipeline: **Load**.\n",
    "\n",
    "Load means:\n",
    "\n",
    "- dlt creates tables in DuckDB (if they do not already exist)\n",
    "- the normalized rows are inserted into those tables\n",
    "- the pipeline records the load in its internal tracking tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aa2dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------- Load rest_api in 1772212573.7407193 ----------------------\n",
      "Jobs: 0/1 (0.0%) | Time: 0.00s | Rate: 0.00/s\n",
      "Memory usage: 207.52 MB (22.90%) | CPU usage: 0.00%\n",
      "\n",
      "--------------------- Load rest_api in 1772212573.7407193 ----------------------\n",
      "Jobs: 0/1 (0.0%) | Time: 0.01s | Rate: 0.00/s\n",
      "Memory usage: 209.77 MB (22.90%) | CPU usage: 0.00%\n",
      "\n"
     ]
    },
    {
     "ename": "PipelineStepFailed",
     "evalue": "Pipeline execution failed at `step=load` when processing package with `load_id=1772212573.7407193` with exception:\n\n<class 'dlt.destinations.exceptions.DestinationConnectionError'>\nConnection with `client_type=DuckDbSqlClient` to `dataset_name=nyc_taxi_data` failed. Please check if you configured the credentials at all and provided the right credentials values. You can be also denied access or your internet connection may be down. The actual reason given is: IO Error: Cannot open file \"/home/hadoop/workspace/notebooks/workshop-01/nyc_yellow_trip_pipeline.duckdb\": Permission denied\n\nPending packages are left in the pipeline and will be re-tried on the next pipeline run. If you pass new data to extract to next run, it will be ignored. Run `dlt pipeline nyc_yellow_trip_pipeline info` for more information or `dlt pipeline nyc_yellow_trip_pipeline drop-pending-packages` to drop pending packages.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIOException\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/dlt/destinations/sql_client.py:479\u001b[39m, in \u001b[36mraise_open_connection_error.<locals>._wrap\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/dlt/destinations/impl/duckdb/sql_client.py:120\u001b[39m, in \u001b[36mDuckDbSqlClient.open_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;129m@raise_open_connection_error\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mopen_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> duckdb.DuckDBPyConnection:\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[38;5;28mself\u001b[39m._conn = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconn_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mborrow_conn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpragmas\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pragmas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m        \u001b[49m\u001b[43mglobal_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_global_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msearch_path\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfully_qualified_dataset_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._conn\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/dlt/destinations/impl/duckdb/configuration.py:131\u001b[39m, in \u001b[36mDuckDbConnectionPool.borrow_conn\u001b[39m\u001b[34m(self, global_config, local_config, pragmas)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._conn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     new_conn = \u001b[43mduckdb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_conn_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m        \u001b[49m\u001b[43mread_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnect_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mIOException\u001b[39m: IO Error: Cannot open file \"/home/hadoop/workspace/notebooks/workshop-01/nyc_yellow_trip_pipeline.duckdb\": Permission denied",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mDestinationConnectionError\u001b[39m                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/dlt/pipeline/pipeline.py:616\u001b[39m, in \u001b[36mPipeline.load\u001b[39m\u001b[34m(self, destination, dataset_name, credentials, workers, raise_on_failed_jobs)\u001b[39m\n\u001b[32m    611\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[32m    612\u001b[39m     signals.intercepted_signals()\n\u001b[32m    613\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.runtime_config.intercept_signals\n\u001b[32m    614\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m nullcontext()\n\u001b[32m    615\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_step\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    617\u001b[39m info: LoadInfo = \u001b[38;5;28mself\u001b[39m._get_step_info(load_step)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/dlt/common/runners/pool_runner.py:208\u001b[39m, in \u001b[36mrun_pool\u001b[39m\u001b[34m(config, run_f)\u001b[39m\n\u001b[32m    207\u001b[39m logger.debug(\u001b[33m\"\u001b[39m\u001b[33mRunning pool\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[43m_run_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    209\u001b[39m     \u001b[38;5;66;03m# raise on signal: safe to do that out of _run_func()\u001b[39;00m\n\u001b[32m    210\u001b[39m     signals.raise_if_signalled()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/dlt/common/runners/pool_runner.py:201\u001b[39m, in \u001b[36mrun_pool.<locals>._run_func\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(run_f, Runnable):\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m     run_metrics = \u001b[43mrun_f\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTExecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/dlt/load/load.py:696\u001b[39m, in \u001b[36mLoad.run\u001b[39m\u001b[34m(self, pool)\u001b[39m\n\u001b[32m    695\u001b[39m             \u001b[38;5;28mself\u001b[39m._step_info_start_load_id(load_id)\n\u001b[32m--> \u001b[39m\u001b[32m696\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_single_package\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m TRunMetrics(\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.load_storage.list_normalized_packages()))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/dlt/load/load.py:598\u001b[39m, in \u001b[36mLoad.load_single_package\u001b[39m\u001b[34m(self, load_id, schema)\u001b[39m\n\u001b[32m    597\u001b[39m \u001b[38;5;28mself\u001b[39m.init_jobs_counter(load_id)\n\u001b[32m--> \u001b[39m\u001b[32m598\u001b[39m running_jobs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minitialize_package\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    599\u001b[39m \u001b[38;5;66;03m# loop until all jobs are processed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/dlt/load/load.py:538\u001b[39m, in \u001b[36mLoad.initialize_package\u001b[39m\u001b[34m(self, load_id, schema, new_jobs)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;66;03m# initialize analytical storage ie. create dataset required by passed schema\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m538\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_destination_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjob_client\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpected_update\u001b[49m\u001b[43m \u001b[49m\u001b[43m:=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_storage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin_schema_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# init job client\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/dlt/destinations/job_client_impl.py:426\u001b[39m, in \u001b[36mSqlJobClientBase.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mSqlJobClientBase\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msql_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    427\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/dlt/destinations/sql_client.py:487\u001b[39m, in \u001b[36mraise_open_connection_error.<locals>._wrap\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    486\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m db_ex.with_traceback(ex.__traceback__) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m DestinationConnectionError(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m, \u001b[38;5;28mself\u001b[39m.dataset_name, \u001b[38;5;28mstr\u001b[39m(ex), ex)\n",
      "\u001b[31mDestinationConnectionError\u001b[39m: Connection with `client_type=DuckDbSqlClient` to `dataset_name=nyc_taxi_data` failed. Please check if you configured the credentials at all and provided the right credentials values. You can be also denied access or your internet connection may be down. The actual reason given is: IO Error: Cannot open file \"/home/hadoop/workspace/notebooks/workshop-01/nyc_yellow_trip_pipeline.duckdb\": Permission denied",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mPipelineStepFailed\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m load_info = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/dlt/pipeline/pipeline.py:224\u001b[39m, in \u001b[36mwith_runtime_trace.<locals>.decorator.<locals>._wrap\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trace:\n\u001b[32m    222\u001b[39m         trace_step = start_trace_step(trace, cast(TPipelineStep, f.\u001b[34m__name__\u001b[39m), \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     step_info = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m step_info\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/dlt/pipeline/pipeline.py:164\u001b[39m, in \u001b[36mwith_state_sync.<locals>.decorator.<locals>._wrap\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    162\u001b[39m should_extract_state = may_extract_state \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.restore_from_destination\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.managed_state(extract_state=should_extract_state):\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/dlt/pipeline/pipeline.py:272\u001b[39m, in \u001b[36mwith_config_section.<locals>.decorator.<locals>._wrap\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrap\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mPipeline\u001b[39m\u001b[33m\"\u001b[39m, *args: Any, **kwargs: Any) -> Any:\n\u001b[32m    266\u001b[39m     \u001b[38;5;66;03m# add section context to the container to be used by all configuration without explicit sections resolution\u001b[39;00m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inject_section(\n\u001b[32m    268\u001b[39m         ConfigSectionContext(\n\u001b[32m    269\u001b[39m             pipeline_name=\u001b[38;5;28mself\u001b[39m.pipeline_name, sections=sections, merge_style=merge_func\n\u001b[32m    270\u001b[39m         )\n\u001b[32m    271\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/dlt/pipeline/pipeline.py:626\u001b[39m, in \u001b[36mPipeline.load\u001b[39m\u001b[34m(self, destination, dataset_name, credentials, workers, raise_on_failed_jobs)\u001b[39m\n\u001b[32m    624\u001b[39m     err_load_id = load_step.current_load_id\n\u001b[32m    625\u001b[39m step_info = \u001b[38;5;28mself\u001b[39m._get_step_info(load_step)\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m PipelineStepFailed(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mload\u001b[39m\u001b[33m\"\u001b[39m, err_load_id, l_ex, step_info) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01ml_ex\u001b[39;00m\n",
      "\u001b[31mPipelineStepFailed\u001b[39m: Pipeline execution failed at `step=load` when processing package with `load_id=1772212573.7407193` with exception:\n\n<class 'dlt.destinations.exceptions.DestinationConnectionError'>\nConnection with `client_type=DuckDbSqlClient` to `dataset_name=nyc_taxi_data` failed. Please check if you configured the credentials at all and provided the right credentials values. You can be also denied access or your internet connection may be down. The actual reason given is: IO Error: Cannot open file \"/home/hadoop/workspace/notebooks/workshop-01/nyc_yellow_trip_pipeline.duckdb\": Permission denied\n\nPending packages are left in the pipeline and will be re-tried on the next pipeline run. If you pass new data to extract to next run, it will be ignored. Run `dlt pipeline nyc_yellow_trip_pipeline info` for more information or `dlt pipeline nyc_yellow_trip_pipeline drop-pending-packages` to drop pending packages."
     ]
    }
   ],
   "source": [
    "load_info = pipeline.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4f8d8b",
   "metadata": {},
   "source": [
    "After this step, the data is fully stored in the database and ready to query.\n",
    "\n",
    "At this point:\n",
    "\n",
    "- The `yellow_trips` table contains NYC Yellow Taxi trip records\n",
    "- Any related child tables contain exploded nested data\n",
    "- Everything is now queryable using `pipeline.dataset()` or SQL\n",
    "\n",
    "This is the moment where the data officially moves from \"pipeline processing\" into a database you can explore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3c3180",
   "metadata": {},
   "source": [
    "## üöÄ Step 7: Run the Full Pipeline\n",
    "\n",
    "Now that we have walked through each step individually, we can run the entire workflow using a single command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80659793",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_info = pipeline.run(nyc_yellow_trip_source())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cd15f5",
   "metadata": {},
   "source": [
    "### What does `pipeline.run()` do?\n",
    "\n",
    "`pipeline.run()` simply combines the three steps we already executed manually:\n",
    "\n",
    "1. **Extract** ‚Äì fetch data from the NYC Yellow Trip API\n",
    "2. **Normalize** ‚Äì convert nested JSON into relational tables\n",
    "3. **Load** ‚Äì write those tables into DuckDB\n",
    "\n",
    "In other words, this:\n",
    "\n",
    "```python\n",
    "pipeline.run(source)\n",
    "```\n",
    "\n",
    "is equivalent to:\n",
    "\n",
    "```python\n",
    "pipeline.extract(source)\n",
    "pipeline.normalize()\n",
    "pipeline.load()\n",
    "```\n",
    "\n",
    "There is no hidden magic. It just runs the full ELT process in order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c383bde6",
   "metadata": {},
   "source": [
    "## üîé Step 8: Inspect the Loaded Data\n",
    "\n",
    "Now that the data is loaded into DuckDB, we can inspect it using `pipeline.dataset()`.\n",
    "\n",
    "This gives us a convenient Python interface for exploring the tables that dlt created, without writing SQL.\n",
    "\n",
    "---\n",
    "\n",
    "### List available tables\n",
    "\n",
    "First, let's see what tables exist in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69af6afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pipeline.dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5b13ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0483d9a",
   "metadata": {},
   "source": [
    "### Preview the yellow_trips table\n",
    "\n",
    "Let's look at the first few rows of the main table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbb3ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds.yellow_trips.df()      # main table\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b53de51",
   "metadata": {},
   "source": [
    "### Basic Data Analysis\n",
    "\n",
    "Let's explore some basic statistics about the NYC Yellow Taxi trips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab3f67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of trips\n",
    "print(f\"Total trips: {len(df)}\")\n",
    "print()\n",
    "\n",
    "# Display data types \n",
    "print(\"Data types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180fddc0",
   "metadata": {},
   "source": [
    "## üí° Conclusion\n",
    "\n",
    "### What we accomplished\n",
    "\n",
    "In this notebook, we built a complete data pipeline that:\n",
    "\n",
    "‚úî Fetches NYC Yellow Taxi trip data from a REST API  \n",
    "‚úî Handles pagination automatically (stops on empty page)  \n",
    "‚úî Normalizes JSON into relational tables  \n",
    "‚úî Loads data into DuckDB  \n",
    "‚úî Provides easy data inspection and analysis  \n",
    "\n",
    "---\n",
    "\n",
    "### What dlt handled for us\n",
    "\n",
    "‚úî API requests with proper pagination  \n",
    "‚úî JSON to relational normalization  \n",
    "‚úî Table creation with proper schemas  \n",
    "‚úî Database loading  \n",
    "‚úî Simple dataset inspection  \n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "‚Ä¢ **Extract** downloads raw data from the API  \n",
    "‚Ä¢ **Normalize** converts JSON into clean relational tables  \n",
    "‚Ä¢ **Load** writes data into the destination database  \n",
    "‚Ä¢ `pipeline.run()` executes all three steps in sequence  \n",
    "‚Ä¢ The resulting data is easily queryable via `pipeline.dataset()` or SQL  \n",
    "\n",
    "You now have a working pipeline processing real NYC Yellow Taxi trip data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92cb8de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
